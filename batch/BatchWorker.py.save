
# Standard library
import os
import re
import shutil

# Third-party libraries
from dotenv import load_dotenv
from flask import Flask, request, jsonify
from flask_socketio import SocketIO
from celery import Celery, current_task
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from typing_extensions import List, TypedDict

# Langchain and related libraries
from langchain import hub
from langchain.document_loaders import (
    TextLoader,
    PyPDFLoader,
    UnstructuredMarkdownLoader,
    Docx2txtLoader,
)
from langchain_text_splitters import CharacterTextSplitter
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document
from langgraph.graph import START, StateGraph
import chromadb
import io
# Local application
from models import Documentt, Status_Embeddings, Base, Status
from smb.SMBConnection import SMBConnection
import tempfile

load_dotenv()

#  Configuraci贸n de Redis
redis_host = os.getenv("REDIS_HOST", "redis")
redis_port = os.getenv("REDIS_PORT", "6379")

#  Configuraci贸n de WebSockets
socketio = SocketIO(message_queue=f"redis://{redis_host}:{redis_port}", cors_allowed_origins="*")

#  Configuraci贸n de Celery
celery_app = Celery('tasks', broker=f"redis://{redis_host}:{redis_port}/0")

# Configuraci贸n de la base de datos

class State(TypedDict):
    question: str
    context: List[Document]
    answer: str
    collection: str

# Configuraci贸n de la base de datos
db_user = os.environ.get("POSTGRES_USER", "admin")
db_password = os.environ.get("POSTGRES_PASSWORD", "password")
db_host = os.environ.get("POSTGRES_HOST", "db")
db_name = os.environ.get("POSTGRES_DB", "rag_saas")
db_port = os.environ.get("POSTGRES_PORT", "5432")
DATABASE_URL = f"postgresql+psycopg2://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}"
SMB_SERVER = os.environ.get("SMB_SERVER")  # IP o hostname de la VM que tiene el sistema de archivos
SMB_PORT = int(os.environ.get("SMB_PORT", "445"))
SMB_USERNAME = os.environ.get("SMB_USERNAME")
SMB_PASSWORD = os.environ.get("SMB_PASSWORD")
SMB_SHARE = os.environ.get("SMB_SHARE")  # Nombre del recurso compartido en la VM remota
SMB_DIRECTORY = os.environ.get("SMB_DIRECTORY", "uploadedDocuments")  # Directorio dentro del recurso compartido
MY_NAME = os.environ.get("MY_NAME", "backend")  # Nombre del cliente para la conexi贸n SMB
REMOTE_NAME = os.environ.get("REMOTE_NAME", "fileserver")  # Nombre de la VM remota en la red


engine = create_engine(DATABASE_URL)
Base.metadata.create_all(engine)
Session = sessionmaker(bind=engine)

def get_loader(file_input, extension):
    """
    Si file_input es una ruta (str), se utiliza directamente.
    Si es un objeto file-like (por ejemplo, io.BytesIO), se escribe su contenido en un archivo temporal.
    """
    # Si ya es una ruta, 煤sala tal cual.
    if isinstance(file_input, str):
        file_path = file_input
    else:
        # Se asume que es un objeto file-like
        suffix = f".{extension}"
        with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
            tmp.write(file_input.read())
            file_path = tmp.name
        # Reinicia el puntero del objeto, por si se necesita m谩s adelante.
        file_input.seek(0)

    if extension == "txt":
        return TextLoader(file_path, encoding="utf-8")
    elif extension == "pdf":
        return PyPDFLoader(file_path)
    elif extension == "md":
        return UnstructuredMarkdownLoader(file_path)
    else:
        raise ValueError(f"Unsupported file format: {extension}")


# Crear nuevo usuario
def embbedings(document_id,extension,collection_name):
    try:
        conn = SMBConnection(SMB_USERNAME, SMB_PASSWORD, MY_NAME, REMOTE_NAME, use_ntlm_v2=True)
        if not conn.connect(SMB_SERVER, SMB_PORT):
            return {'message': 'Error al conectar con el servidor de archivos'}, 500

        file_obj = io.BytesIO()
        conn.retrieveFile(SMB_SHARE,"document_{document_id}.{extension}", file_obj)
        file_obj.seek(0)

    except Exception as e:
        return {'message': f'Error al subir el archivo: {str(e)}'}, 500
    
    loader = get_loader(file_obj, extension)
    documents = loader.load()
    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
    docs = text_splitter.split_documents(documents)

    embeddings = GoogleGenerativeAIEmbeddings(
        model=os.getenv("GOOGLE_EMBEDDING_MODEL", "models/embedding-001"),
        google_api_key=os.getenv("GOOGLE_API_KEY")
    )

    chroma_path = "/chroma_db" 

    Chroma.from_documents(docs, embeddings, persist_directory=chroma_path, collection_name=collection_name)

    print(f" Se han guardado {len(docs)} fragmentos en ChromaDB.")

    session = Session()
    try:
        doc = session.query(Documentt).get(document_id)
        if doc:
            doc.embbedings_status = Status_Embeddings.COMPLETED
            session.commit()
    except Exception as e:
        session.rollback()
        raise e
    finally:
        session.close()


class State(TypedDict):
    question: str
    context: List[Document]
    answer: str
    collection: str

def retrieve(state: State):
    chroma_path = os.getenv("CHROMA_DB_PATH", "/chroma_db")  # Cargar ruta desde el .env
    collection_name = state["collection"]

    embeddings = GoogleGenerativeAIEmbeddings(
    model=os.getenv("GOOGLE_EMBEDDING_MODEL", "models/embedding-001"),
    google_api_key=os.getenv("GOOGLE_API_KEY")
    )

    vector_store = Chroma(
        persist_directory=chroma_path,
        embedding_function=embeddings,
        collection_name=collection_name
    )

    retrieved_docs = vector_store.similarity_search(state["question"])

    return {"context": retrieved_docs}

def generate(state: State):

    llm = ChatGoogleGenerativeAI(
        model=os.getenv("GEMINI_MODEL", "gemini-pro"),
        google_api_key=os.getenv("GOOGLE_API_KEY")
    )

    prompt = hub.pull("rlm/rag-prompt")

    docs_content = "\n\n".join(doc.page_content for doc in state["context"])
    messages = prompt.invoke({"question": state["question"], "context": docs_content})
    response = llm.invoke(messages)
    return {"answer": response.content}

def question(data):
    graph_builder = StateGraph(State).add_sequence([retrieve, generate])
    graph_builder.add_edge(START, "retrieve")
    graph = graph_builder.compile()

    response = graph.invoke({"question": data["question"], "collection": data["collection"]})

    return {"respuesta": response["answer"]}

#  Tarea en Segundo Plano con Celery
@celery_app.task(name='process.sms', queue="allqueue", bin=True)
def long_running_task(data):
    chroma_path = "/chroma_db"  # or os.getenv("CHROMA_DB_PATH")
    client = chromadb.PersistentClient(path=chroma_path)


    collections = client.list_collections()
    exists = data["collection"] in collections

    if not exists:
        embbedings(data["id"],data["extension"],data["collection"])

    respuesta = question(data)

    print(respuesta["respuesta"])
    socketio.emit("task_update", {"task_id": current_task.request.id, "message": respuesta["respuesta"]})
    return f"Task completed"



